"""eval.ipynb

Automatically generated by Colab.

# Preliminary
"""
import os
import numpy as np
import torch
import librosa
from PIL import Image
import pandas as pd
from multiprocessing import Pool
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor
import wav2clip
import clip
from pytorch_fid import fid_score


# Load models
device = "cuda" if torch.cuda.is_available() else "cpu"
wav2clip_model = wav2clip.get_model().to(device)
clip_model, clip_preprocess = clip.load("ViT-B/32", device=device)

# Paths
selected_classes_csv = "data/VGGSound/filtered_vggsound.csv"
zero_shot_classes_csv = "data/VGGSound/zero_shot.csv"
generated_dir = "output/original"
generated_img_dir = os.path.join(generated_dir, "image")
generated_audio_dir = os.path.join(generated_dir, "audio")
zero_shot_generated_img_dir = os.path.join(generated_dir,"zero_shot", "image")
zero_shot_generated_audio_dir = os.path.join(generated_dir,"zero_shot", "audio")
reference_img_dir = "data/reference/"

""" Preliminary """
# Load dataframe with header ['Youtube ID', 'start seconds', label', 'train/test split']
df = pd.read_csv(selected_classes_csv, header=0)
df['filename'] = df.apply(lambda row: f"{row['YouTube ID']}_{int(row['start seconds']):06d}", axis=1)
labels = df['label'].unique().tolist()  # List of all possible labels for reference

# Initiate lists
generated_audio_paths, generated_image_paths, gen_labels = [], [], []
reference_image_paths = {}

# Create generated data lists
audios, images = set(), set()
audios = audios | set([file_path[:-4] for file_path in os.listdir(generated_audio_dir)])
images = images | set([file_path[:-4] for file_path in os.listdir(generated_img_dir)])
samples = audios & images

for sample in samples:
  label = df.loc[df['filename'] == sample.rpartition('_')[0], 'label'].values[0]
  generated_image_paths.append(os.path.join(generated_img_dir, sample + ".png"))
  generated_audio_paths.append(os.path.join(generated_audio_dir, sample + ".wav"))
  gen_labels.append(label)

# Create reference image dictionary
sum_ref_img = 0
reference_image_dict = {label_name : [] for label_name in labels}
for file_name in os.listdir(reference_img_dir):
  label = file_name.split('_')[0]
  reference_image_dict[label].append(os.path.join(reference_img_dir, file_name))
  sum_ref_img += 1

""" Embeddings """
# Wav2CLIP Audio Embedding
@torch.no_grad()
def get_audio_embeddings(audio_paths, audio_encoder, device, target_length=16000, batch_size=32):
    audio_features_list = []
    def load_and_preprocess(path):
        track, _ = librosa.load(path, sr=16000, dtype=np.float32)
        if len(track) < target_length:
            track = np.pad(track, (0, target_length - len(track)))  # Pad
        else:
            track = track[:target_length]  # Trim
        return track

    # Parallel audio loading using ThreadPoolExecutor
    with ThreadPoolExecutor(max_workers=8) as executor:
        all_tracks = list(tqdm(executor.map(load_and_preprocess, audio_paths), total=len(audio_paths), desc="Loading Audio"))
    # Batch processing with GPU acceleration
    for i in tqdm(range(0, len(all_tracks), batch_size), desc="Embedding Batches"):
        batch_tracks = np.array(all_tracks[i: i + batch_size], dtype=np.float32)
        # âœ… Convert back to NumPy before embedding (wav2clip requires this)
        batch_embeddings = audio_encoder(torch.tensor(batch_tracks).to(device))
        # Move to GPU for normalization and further processing
        batch_embeddings = torch.tensor(batch_embeddings, dtype=torch.float32).to(device)
        batch_embeddings = batch_embeddings / batch_embeddings.norm(dim=-1, keepdim=True)  # Normalize
        audio_features_list.append(batch_embeddings)
    return torch.cat(audio_features_list, dim=0)

# CLIP Image Embedding
def preprocess_image(image_path, preprocess_fn):
    image = Image.open(image_path).convert("RGB")
    return preprocess_fn(image)

@torch.no_grad()
def get_image_embeddings(image_paths, model, preprocess_fn, batch_size=32):
    image_features_list = []
    # Parallel image loading
    with ThreadPoolExecutor(max_workers=8) as executor:
        all_images = list(tqdm(executor.map(lambda path: preprocess_image(path, preprocess_fn), image_paths),  total=len(image_paths), desc="Loading Images"))
    # Batch processing
    for i in tqdm(range(0, len(all_images), batch_size), desc="Embedding Batches"):
        batch_images = torch.stack(all_images[i: i + batch_size]).to(device)
        batch_embeddings = model.encode_image(batch_images)  # For CLIP
        # Normalize embeddings
        batch_embeddings = batch_embeddings / batch_embeddings.norm(dim=-1, keepdim=True)
        image_features_list.append(batch_embeddings)
    return torch.cat(image_features_list, dim=0)

#CLIP Text Embedding
@torch.no_grad()
def get_text_embeddings(label_texts, device):
    text_tokens = clip.tokenize(label_texts).to(device)  # Tokenize the text labels
    text_embeddings = clip_model.encode_text(text_tokens)  # Get text embeddings
    return text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)  # Normalize embeddings

""" Calculate Embeddings """
# Generated audio & image embeddings
print("Calculating audio embeddings")
gen_audio_embed = get_audio_embeddings(generated_audio_paths, wav2clip_model, device)
print("Calculating generated image embeddings")
gen_image_embed = get_image_embeddings(generated_image_paths, clip_model, clip_preprocess)

# Reference embeddings
print(f"Calculating 'a photo of *' label embedding")
sent_labels = [f"a photo of {label}" for label in labels]
sent_embed = get_text_embeddings(sent_labels, device)

print("Calculating reference image embeddings")
ref_image_dict_embed = {label : get_image_embeddings(reference_image_dict[label], clip_model, clip_preprocess) for label in labels}
ref_image_embed_lst = torch.cat(list(ref_image_dict_embed.values()))
ref_labels_lst = [label for label, ref in reference_image_dict.items() for _ in range(len(ref))]

label_to_index = {label: idx for idx, label in enumerate(labels)}
numeric_gen_labels = torch.tensor([label_to_index[label] for label in gen_labels], device=device)
numeric_ref_labels = torch.tensor([label_to_index[label] for label in ref_labels_lst], device=device)

""" Evaluation Metrics """
# Generalized function to calculate similarity-based scores for AIS and IIS
@torch.no_grad()
def calculate_similarity_score(target_embeddings, reference_embeddings, target_labels, reference_labels):
    # Compute similarity matrix
    similarity_matrix = target_embeddings @ reference_embeddings.T  # Shape: (N, M)
    # Diagonal similarity scores (correct matches)
    target_values = similarity_matrix.diag()
    # Mask for incorrect labels
    same_label_mask = target_labels.unsqueeze(1) == reference_labels  # Shape: (N, M)
    # Get reference values
    reference_values = []
    for i, row in enumerate(same_label_mask):
        false_indices = torch.nonzero(~row, as_tuple=True)[0] # Indices where the value is False
        random_idx = false_indices[torch.randint(0, len(false_indices), (1,))].item()
        reference_values.append(similarity_matrix[i, random_idx])
    # Count valid matches
    valid_matches = (target_values > torch.tensor(reference_values, device=device)).sum().item()
    # Final similarity score
    return valid_matches / target_embeddings.shape[0]

# Calculate AIS
@torch.no_grad()
def calculate_ais(gen_image_embed, gen_audio_embed, gen_labels, num_calculations=1):
    ais_score = 0
    for _ in range(num_calculations):
        ais_score += calculate_similarity_score(gen_image_embed, gen_audio_embed, gen_labels, gen_labels)
    return ais_score / num_calculations

# Calculate IIS
@torch.no_grad()
def calculate_iis(gen_image_embed, ref_image_dict_embed, numeric_gen_labels, gen_labels, num_calculations=1):
    iis_score = 0
    for _ in range(num_calculations):
        ref_image_embed = []
        for label in gen_labels:
            rand_idx = torch.randint(0, len(ref_image_dict_embed[label]), (1,)).item()
            ref_image_embed.append(ref_image_dict_embed[label][rand_idx])
        ref_image_embed = torch.stack(ref_image_embed).to(device)
        iis_score += calculate_similarity_score(gen_image_embed, ref_image_embed, numeric_gen_labels, numeric_gen_labels)
    return iis_score / num_calculations

# Calculate AIC
@torch.no_grad()
def calculate_aic(gen_image_embed, label_embed, gen_labels, labels):
    # Image label prediction
    similarity_matrix = gen_image_embed @ label_embed.T
    predicted_indices = similarity_matrix.argmax(dim=1)
    valid_matches = 0
    for predicted_index, true_label in zip(predicted_indices, gen_labels):
        # print(f"Predicted Label: {gen_labels[predicted_index]} ; True Label: {true_label}")
        if labels[predicted_index] == true_label:
            valid_matches += 1
    return valid_matches / len(gen_labels)

@torch.no_grad()
def recall_at_k(target_embeddings, reference_embeddings, target_labels, reference_labels, k=5):
    # Calculate top k matches
    similarity_matrix = target_embeddings @ reference_embeddings.T  # Shape: (N, M)
    top_k_indices = torch.argsort(-similarity_matrix, dim=1)[:, :k]  # Negative sign for descending order
    # Calculate R@k score
    valid_matches = 0
    for row in range(top_k_indices.shape[0]):
        if target_labels[row] in [reference_labels[idx] for idx in top_k_indices[row]]:
            valid_matches += 1
    return valid_matches / target_embeddings.shape[0]

# FID Metric Calculation
@torch.no_grad()
def calculate_fid(real_images_path, generated_images_path):
    fid_value = fid_score.calculate_fid_given_paths(
        [real_images_path, generated_images_path],
        batch_size=50,
        device="cuda" if torch.cuda.is_available() else "cpu",
        dims=2048)  # Standard InceptionV3 feature size
    return fid_value

""" Calculate Metrics """
gen_audio_embed = gen_audio_embed.to(gen_image_embed.dtype)
num_calculations = 20
# Calculate AIS
ais_score = calculate_ais(gen_image_embed, gen_audio_embed, numeric_gen_labels, num_calculations)
print(f"AIS Score: {ais_score*100:.2f}")
# Calculate IIS
iis_score = calculate_iis(gen_image_embed, ref_image_dict_embed, numeric_gen_labels, gen_labels, num_calculations)
print(f"IIS Score: {iis_score*100:.2f}")
# Calculate AIC
aic_score = calculate_aic(gen_image_embed, sent_embed, gen_labels, labels)
print(f"AIC Score: {aic_score*100:.2f}")
# Calculate R@1,3,5,10
print("Recall @ k Scores")
r1_score = recall_at_k(gen_image_embed, sent_embed, gen_labels, labels, k=1)
r3_score = recall_at_k(gen_image_embed, sent_embed, gen_labels, labels, k=3)
r5_score = recall_at_k(gen_image_embed, sent_embed, gen_labels, labels, k=5)
r10_score = recall_at_k(gen_image_embed, sent_embed, gen_labels, labels, k=10)
print(f"Image-Text: R@1/3/5/10 Score: {r1_score*100:.2f} / {r3_score*100:.2f} / {r5_score*100:.2f} / {r10_score*100:.2f}")
# Calculate FID
fid_value = calculate_fid(reference_fid_dir, generated_img_dir)
print(f"FID Score: {fid_value:.2f}")



""" Zero Shot """
# Create dataframe
df = pd.read_csv(selected_classes_csv, header=0)
df['filename'] = df.apply(lambda row: f"{row['YouTube ID']}_{int(row['start seconds']):06d}", axis=1)
labels = df['label'].unique().tolist()  # List of all possible labels for reference

# Initiate lists
generated_audio_paths, generated_image_paths, gen_labels = [], [], []
reference_image_paths = {}

# Create generated data lists
audios, images = set(), set()
audios = audios | set([file_path[:-4] for file_path in os.listdir(generated_audio_dir)])
images = images | set([file_path[:-4] for file_path in os.listdir(generated_img_dir)])

for sample in audios & images:
  label = df.loc[df['filename'] == sample.rpartition('_')[0], 'label'].values[0]
  generated_image_paths.append(os.path.join(generated_img_dir, sample + ".png"))
  generated_audio_paths.append(os.path.join(generated_audio_dir, sample + ".wav"))
  gen_labels.append(label)

# Precompute embeddings
print("Calculating audio embeddings")
gen_audio_embed = get_audio_embeddings(generated_audio_paths, wav2clip_model, device)
print("Calculating generated image embeddings")
gen_image_embed = get_image_embeddings(generated_image_paths, clip_model, clip_preprocess)

# Calculate zero shot score
r1_score = recall_at_k(gen_image_embed, sent_embed, gen_labels, labels, k=1)
print(f"Zero Shot Classification Score: {r1_score*100:.2f}")